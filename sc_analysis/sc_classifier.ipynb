{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447efff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ls/linoym/.conda/envs/airrport/lib/python3.11/site-packages/celltypist/classifier.py:11: FutureWarning: `__version__` is deprecated, use `importlib.metadata.version('scanpy')` instead\n",
      "  from scanpy import __version__ as scv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redirecting output to log file: scanpy_analysis.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'feature_types' as categorical\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: saving figure to file figures/umap_leiden_clusters.png\n",
      "WARNING: saving figure to file figures/pca_pca_clusters.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìÇ Storing models in /home/ls/linoym/.celltypist/data/models\n",
      "üíæ Total models to download: 1\n",
      "‚è© Skipping [1/1]: Immune_All_Low.pkl (file exists)\n",
      "üëÄ Invalid expression matrix in `.X`, expect log1p normalized expression to 10000 counts per cell; will use `.raw.X` instead\n",
      "üî¨ Input data has 11084 cells and 25687 genes\n",
      "üîó Matching reference genes in the model\n",
      "üß¨ 5618 features used for prediction\n",
      "‚öñÔ∏è Scaling input data\n",
      "üñãÔ∏è Predicting labels\n",
      "‚úÖ Prediction done!\n",
      "üëÄ Detected a neighborhood graph in the input object, will run over-clustering on the basis of it\n",
      "‚õìÔ∏è Over-clustering input data with resolution set to 10\n",
      "üó≥Ô∏è Majority voting the predictions\n",
      "‚úÖ Majority voting done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: saving figure to file figures/umap_celltypist_annotation.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "... storing 'feature_types' as categorical\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 544\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    543\u001b[39m     classifier_data = classifier_table(sc_rna_seq,df_airrport_igblast_unified,df_vdj ,PUBLICNESS_PATH,cell_type_col = \u001b[33m'\u001b[39m\u001b[33mpredicted_cell_type\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 499\u001b[39m, in \u001b[36mbuild_model\u001b[39m\u001b[34m(classifier_table)\u001b[39m\n\u001b[32m    498\u001b[39m importances = model.feature_importances_\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m feature_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfeature\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimportance\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportances\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[38;5;28mprint\u001b[39m(feature_df.sort_values(by=\u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).head(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/airrport/lib/python3.11/site-packages/pandas/core/frame.py:782\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    781\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    783\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/airrport/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/airrport/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/airrport/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 557\u001b[39m\n\u001b[32m    554\u001b[39m         close_logging(log_handle)\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 554\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# This block will run NO MATTER WHAT (success or error),\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;66;03m# ensuring your log file is always closed properly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[43mclose_logging\u001b[49m(log_handle)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 554\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# This block will run NO MATTER WHAT (success or error),\u001b[39;00m\n\u001b[32m    553\u001b[39m     \u001b[38;5;66;03m# ensuring your log file is always closed properly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[43mclose_logging\u001b[49m(log_handle)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1697\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:634\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1112\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:1090\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_bundle\\\\pydevd_cython.pyx:494\u001b[39m, in \u001b[36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/airrport/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/airrport/lib/python3.11/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/airrport/lib/python3.11/threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    627\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/airrport/lib/python3.11/threading.py:331\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    333\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import celltypist\n",
    "import pandas as pd\n",
    "from typing import TextIO\n",
    "# Import AnnData for cell type annotation\n",
    "from anndata import AnnData\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb # <-- NEW IMPORT\n",
    "\n",
    "\n",
    "def setup_logging(logfile: str) -> TextIO:\n",
    "    \"\"\"\n",
    "    Redirects stdout and stderr to a log file.\n",
    "    \n",
    "    Args:\n",
    "        logfile: The path to the log file.\n",
    "        \n",
    "    Returns:\n",
    "        The file handle for the opened log file.\n",
    "    \"\"\"\n",
    "    print(f\"Redirecting output to log file: {logfile}\")\n",
    "    log_file_handle = open(logfile, \"w\")\n",
    "    sys.stdout = log_file_handle\n",
    "    sys.stderr = log_file_handle\n",
    "    print(\"Starting Scanpy pipeline...\")\n",
    "    return log_file_handle\n",
    "\n",
    "def close_logging(log_file_handle: TextIO):\n",
    "    \"\"\"\n",
    "    Closes the log file and restores stdout/stderr.\n",
    "    \n",
    "    Args:\n",
    "        log_file_handle: The file handle returned by setup_logging.\n",
    "    \"\"\"\n",
    "    print(\"Analysis pipeline completed successfully.\")\n",
    "    sys.stdout.close()\n",
    "    # Restore standard output\n",
    "    sys.stdout = sys.__stdout__\n",
    "    sys.stderr = sys.__stderr__\n",
    "    print(f\"Log file created. Check 'scanpy_analysis.log' for details.\")\n",
    "\n",
    "def load_data(data_dir: str, airrport_path: str, vdj_path: str, igblast_path: str) -> tuple[AnnData, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads the 10x Genomics data and renames barcodes.\n",
    "    Loads AIRRPORT dat (parquet file)\n",
    "    Loads Igblast data and merge with AIRRPORT data by seq_id \n",
    "    Load vdj-seq data (refernce)\n",
    "    \"\"\"\n",
    "    print(\"--- Loading single cells Data ---\")\n",
    "    adata = sc.read_10x_mtx(data_dir, var_names='gene_symbols', cache=True)\n",
    "    # replace -1 from barcodes with empthy string\n",
    "    adata.obs_names = adata.obs_names.str.replace('-1', '', regex=False)\n",
    "    print(f\"Loaded {adata.n_obs} cells and {adata.n_vars} genes.\")\n",
    "\n",
    "    \n",
    "    ### AIRRPORT results ###\n",
    "    print(f\"--- Loading AIRRPORT data from {airrport_path} ---\")\n",
    "    df_airrport = pd.read_parquet(airrport_path)\n",
    "    # Clean Cell Barcode (CB)\n",
    "    df_airrport['CB'] = df_airrport['CB'].astype(str).str.replace('Z:', '', regex=False)\n",
    "    df_airrport['CB'] = df_airrport['CB'].str.replace('*', '-1', regex=False)\n",
    "    df_airrport['CB'] = df_airrport['CB'].str.replace('-1', '', regex=False)\n",
    "\n",
    "    ### VDJ (CDR3 sequencing for reference) ###\n",
    "    print(f\"--- Loading VDJ data from {vdj_path} ---\")\n",
    "    df_vdj = pd.read_csv(vdj_path)\n",
    "    # Add ‚Äúvdj‚Äù before each column name\n",
    "    df_vdj = df_vdj.add_prefix('vdj_')\n",
    "    df_vdj['vdj_barcode'] = df_vdj['vdj_barcode'].str.replace(\"-1$\", \"\", regex=True)\n",
    "\n",
    "    ### IgBlast for AIRRPORT results ###\n",
    "    print(f\"--- Loading IgBlast data from {igblast_path} ---\")\n",
    "    df_airrport_igblast = pd.read_csv(igblast_path, sep='\\t')\n",
    "    # Add ‚ÄúIgBlast‚Äù before each column name\n",
    "    df_airrport_igblast = df_airrport_igblast.add_prefix('IgBlast_')\n",
    "    \n",
    "    # Select summary columns\n",
    "    df_airrport_igblast_summary = df_airrport_igblast[[\n",
    "        \"IgBlast_sequence_id\", \"IgBlast_sequence_aa\", \"IgBlast_sequence_alignment_aa\",\n",
    "        \"IgBlast_cdr3\", \"IgBlast_cdr3_aa\", \"IgBlast_v_support\", \"IgBlast_j_support\",\n",
    "        \"IgBlast_v_identity\", \"IgBlast_j_identity\"\n",
    "    ]]\n",
    "\n",
    "    ### Left join to IgBlast with AIRRPORT ###\n",
    "    print(\"Joining IgBlast and AIRRPORT data...\")\n",
    "    df_airrport_igblast_unified = pd.merge(\n",
    "        df_airrport_igblast_summary,\n",
    "        df_airrport,\n",
    "        left_on=\"IgBlast_sequence_id\",\n",
    "        right_on=\"seq_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    ### Filter by conditions in order to remove noise ###\n",
    "    print(\"Filtering results based on strict conditions...\")\n",
    "    cdr3_strict_conditions = df_airrport_igblast_unified[\n",
    "        (df_airrport_igblast_unified['IgBlast_v_support'] < 1e-05) &\n",
    "        (df_airrport_igblast_unified['IgBlast_v_identity'] >= 90) &\n",
    "        (df_airrport_igblast_unified['IgBlast_j_support'] < 1e-05) &\n",
    "        (df_airrport_igblast_unified['IgBlast_j_identity'] >= 90)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Found {len(cdr3_strict_conditions)} high-confidence (strict conditions of IgBlast) AIRR-seq entries.\")\n",
    "\n",
    "\n",
    "    return adata, df_airrport, df_vdj, df_airrport_igblast, cdr3_strict_conditions, df_airrport_igblast_unified\n",
    "\n",
    "def analysis_logs(adata, df_airrport, df_vdj, df_airrport_igblast, cdr3_strict_conditions, classifier_table):\n",
    "    print(f\"--- AIRRPORT Data Analysis ---\")\n",
    "    if \"CDR3_match\" in df_airrport.columns:\n",
    "        unique_airrport_cdr3_count = df_airrport['CDR3_match'].nunique()\n",
    "        print(f\"Unique values in df_airrport['CDR3_match']: {unique_airrport_cdr3_count}\")\n",
    "        airrport_cdr3_set = set(df_airrport['CDR3_match'].dropna())\n",
    "    else:\n",
    "        print(\"Column 'CDR3_match' not found in df_airrport. Skipping count.\")\n",
    "        airrport_cdr3_set = set()\n",
    "\n",
    "    print(f\"--- VDJ Data Analysis ---\")\n",
    "    if \"vdj_cdr3\" in df_vdj.columns:\n",
    "        unique_vdj_cdr3_count = df_vdj['vdj_cdr3'].nunique()\n",
    "        print(f\"Unique values in df_vdj['vdj_cdr3']: {unique_vdj_cdr3_count}\")\n",
    "        vdj_cdr3_set = set(df_vdj['vdj_cdr3'].dropna())\n",
    "    else:\n",
    "        print(\"Column 'vdj_cdr3' not found in df_vdj. Skipping count.\")\n",
    "        vdj_cdr3_set = set()\n",
    "\n",
    "    print(f\"--- Common Sequence Analysis ---\")\n",
    "    if \"CDR3_match\" in df_airrport.columns and \"vdj_cdr3\" in df_vdj.columns:\n",
    "        common_sequences = airrport_cdr3_set.intersection(vdj_cdr3_set)\n",
    "        print(f\"Common sequences between df_airrport['CDR3_match'] and df_vdj['vdj_cdr3']: {len(common_sequences)}\")\n",
    "    else:\n",
    "        print(\"Skipping common sequence count due to missing column(s).\")\n",
    "        \n",
    "    # --- NEW SECTION: Classifier Table Analysis ---\n",
    "    print(f\"--- Classifier Table Analysis ---\")\n",
    "    if classifier_table is not None and not classifier_table.empty:\n",
    "        # 1. Unique CDR3 count in classifier_table\n",
    "        # (Assuming the column name in classifier_table is 'cdr3' based on previous code, \n",
    "        # but checking for 'CDR3_match' just in case)\n",
    "        cdr3_col = 'cdr3' if 'cdr3' in classifier_table.columns else 'CDR3_match'\n",
    "        \n",
    "        if cdr3_col in classifier_table.columns:\n",
    "            unique_cdr3_classifier = classifier_table[cdr3_col].nunique()\n",
    "            print(f\"Unique CDR3 sequences in classifier_table: {unique_cdr3_classifier}\")\n",
    "            \n",
    "            # 2. Unique CB count for those sequences\n",
    "            # This is just the total unique CBs in the table, as every row has a CB.\n",
    "            if 'CB' in classifier_table.columns:\n",
    "                unique_cb_classifier = classifier_table['CB'].nunique()\n",
    "                print(f\"Unique Cell Barcodes (CB) in classifier_table: {unique_cb_classifier}\")\n",
    "        else:\n",
    "             print(f\"Warning: Neither 'cdr3' nor 'CDR3_match' found in classifier_table.\")\n",
    "\n",
    "        # 3. Count of sequences that are BOTH 'in_vdj' AND 'T cells'\n",
    "        # We need to check if the required columns exist first.\n",
    "        required_cols = ['label', 'cdr3'] # We can use the 'label' column we created earlier\n",
    "        if all(col in classifier_table.columns for col in required_cols):\n",
    "             # The 'label' column already holds (is_t_cell & is_in_vdj)\n",
    "             # We want unique CDR3s that have at least one True label.\n",
    "             true_label_cdr3s = classifier_table.loc[classifier_table['label'] == True, cdr3_col].nunique()\n",
    "             print(f\"Unique CDR3s that are both 'in_vdj' and 'T cell': {true_label_cdr3s}\")\n",
    "        elif 'in_vdj' in classifier_table.columns and 'cell_type' in classifier_table.columns and cdr3_col in classifier_table.columns:\n",
    "            # Fallback if 'label' column hasn't been created yet\n",
    "            is_t_cell = classifier_table['cell_type'].str.contains(\"T cells\", case=False, na=False)\n",
    "            is_in_vdj = classifier_table['in_vdj'] == True\n",
    "            true_label_cdr3s = classifier_table.loc[is_t_cell & is_in_vdj, cdr3_col].nunique()\n",
    "            print(f\"Unique CDR3s that are both 'in_vdj' and 'T cell': {true_label_cdr3s}\")\n",
    "        else:\n",
    "             print(\"Skipping T-cell/VDJ intersection count due to missing columns (need 'in_vdj', 'cell_type', and 'cdr3').\")\n",
    "\n",
    "    else:\n",
    "        print(\"classifier_table is empty or None.\")\n",
    "\n",
    "\n",
    "def preprocess_sc_data(adata: AnnData) -> AnnData:\n",
    "    \"\"\"\n",
    "    Runs standard preprocessing (filtering, normalization, HVGs).\n",
    "    \n",
    "    Args:\n",
    "        adata: The AnnData object to preprocess.\n",
    "        \n",
    "    Returns:\n",
    "        The preprocessed AnnData object.\n",
    "    \"\"\"\n",
    "    print(\"--- Standard Preprocessing for Single cells ---\")\n",
    "    \n",
    "    # Basic filtering (adjust thresholds as needed for your specific tissue)\n",
    "    # This removes any cell that detected fewer than 200 genes.\n",
    "    # Why? Cells with very few detected genes are often dead cells, empty droplets\n",
    "    # (identifying background noise rather than a real cell), or failed library preparation.\n",
    "    sc.pp.filter_cells(adata, min_genes=200)\n",
    "    # This removes any gene that was detected in fewer than 3 cells across the entire dataset.\n",
    "    # Why? deeply rarely expressed genes (e.g., found in only 1 or 2 cells out of thousands) provide little statistical power for clustering or differential expression and increase computational noise.\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "    print(f\"Data shape after filtering: {adata.n_obs} cells x {adata.n_vars} genes\")\n",
    "\n",
    "    # Mitochondrial gene filtering\n",
    "    # High mitochondrial DNA (mtDNA) percentage is a classic sign of a stressed or dying cell, and these are usually removed from analysis.\n",
    "    adata.var['mito_genes'] = adata.var_names.str.startswith('MT-')  # MT- for human\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=['mito_genes'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "    # Filter cells based on mitochondrial content (e.g., < 10% or < 20%)\n",
    "    # Adjust this threshold based on your data's distribution (plot with sc.pl.violin)\n",
    "    print(f\"Cells before mitochondrial gene filtering: {adata.n_obs}\")\n",
    "    adata = adata[adata.obs.pct_counts_mito_genes < 20, :]\n",
    "    print(f\"Cells after mitochondrial gene filtering: {adata.n_obs}\")\n",
    "\n",
    "    # Normalization and Scaling\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    \n",
    "    # Identify highly variable genes for dimensionality reduction\n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "    \n",
    "    # Make a copy of raw data for later use (e.g., CellTypist prefers it)\n",
    "    # This MUST be done AFTER normalization/log1p but BEFORE subsetting and scaling.\n",
    "    adata.raw = adata \n",
    "    \n",
    "    # 2. Subset to highly variable genes\n",
    "    # This subsets the main adata object for clustering.\n",
    "    adata = adata[:, adata.var.highly_variable]\n",
    "    print(f\"Subsetting to {adata.n_vars} highly variable genes.\")\n",
    "\n",
    "    # 3. Scale data to unit variance and 0 mean.\n",
    "    # This is done LAST, only on the highly variable genes.\n",
    "    sc.pp.scale(adata, max_value=10) \n",
    "    \n",
    "    return adata\n",
    "\n",
    "def cluster_and_embed(adata: AnnData) -> AnnData:\n",
    "    \"\"\"\n",
    "    Runs PCA, neighborhood graph, UMAP, and Leiden clustering.\n",
    "    \n",
    "    Args:\n",
    "        adata: The preprocessed AnnData object.\n",
    "        \n",
    "    Returns:\n",
    "        The clustered and embedded AnnData object.\n",
    "    \"\"\"\n",
    "    print(\"--- Clustering and Embedding ---\")\n",
    "    # PCA and Neighborhood graph\n",
    "    sc.tl.pca(adata, svd_solver='arpack')\n",
    "    sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)\n",
    "    #Clustering and Embedding\n",
    "    sc.tl.umap(adata)\n",
    "    sc.tl.leiden(adata, resolution=0.5) # Adjust resolution for more/fewer clusters\n",
    "    return adata\n",
    "\n",
    "def save_clustering_plots(adata: AnnData):\n",
    "    \"\"\"\n",
    "    Saves UMAP and PCA plots colored by Leiden cluster.\n",
    "    \n",
    "    Args:\n",
    "        adata: The clustered AnnData object.\n",
    "    \"\"\"\n",
    "    print(\"Saving clustering plots...\")\n",
    "    sc.pl.umap(adata, color=['leiden'], save=\"_leiden_clusters.png\", show=False)\n",
    "    sc.pl.pca(adata, color='leiden', save=\"_pca_clusters.png\", show=False)\n",
    "\n",
    "def annotate_cell_types(adata: AnnData, model_name: str) -> AnnData:\n",
    "    \"\"\"\n",
    "    Annotates cell types using CellTypist.\n",
    "    \n",
    "    Args:\n",
    "        adata: The clustered AnnData object.\n",
    "        model_name: The name of the CellTypist model to use.\n",
    "        \n",
    "    Returns:\n",
    "        The AnnData object with 'predicted_cell_type' in .obs.\n",
    "    \"\"\"\n",
    "    print(\"--- Cell Type Annotation (CellTypist) ---\")\n",
    "       \n",
    "    # Download model (e.g., for immune cells)\n",
    "    print(f\"Downloading/loading model: {model_name}\")\n",
    "    model = celltypist.models.download_models(model=model_name)\n",
    "    \n",
    "    # Predict cell types\n",
    "    # Note: CellTypist automatically uses adata.raw if available, which is good practice.\n",
    "    print(\"Running cell type prediction...\")\n",
    "    predictions = celltypist.annotate(adata, model=model, majority_voting=True)\n",
    "    \n",
    "    # Add predictions to AnnData object\n",
    "    # We use the 'majority_voting' result for cleaner clusters\n",
    "    adata.obs['predicted_cell_type'] = predictions.predicted_labels['majority_voting']\n",
    "    return adata\n",
    "\n",
    "def save_annotation_results(adata: AnnData):\n",
    "    \"\"\"\n",
    "    Saves the UMAP plot colored by cell type and a CSV of annotations.\n",
    "    \n",
    "    Args:\n",
    "        adata: The annotated AnnData object.\n",
    "    \"\"\"\n",
    "    print(\"Saving CellTypist annotation plot...\")\n",
    "    sc.pl.umap(adata, color='predicted_cell_type', save=\"_celltypist_annotation.png\", show=False)\n",
    "    \n",
    "    # Optional: Save a CSV of just the barcodes and their new cell types for easy viewing later\n",
    "    print(\"Saving annotation CSV...\")\n",
    "    adata.obs[['leiden', 'predicted_cell_type']].to_csv(\"cell_type_annotations.csv\")\n",
    "\n",
    "def classifier_table(\n",
    "    adata: AnnData,\n",
    "    df_airrport_igblast_unified: pd.DataFrame,\n",
    "    df_vdj: pd.DataFrame,\n",
    "    publicness_file_path: str,\n",
    "    cell_type_col: str = 'predicted_labels'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds the classifier table by joining AIRR, IgBlast, publicness,\n",
    "    gene expression, and cell type data.\n",
    "    \n",
    "    Args:\n",
    "        df_airrport: DataFrame from AIRRPORT (e.g., df_airrport_SRX10124718)\n",
    "        df_igblast: DataFrame from IgBlast (e.g., df_airrport_igblast_..._unified)\n",
    "        adata: Your preprocessed AnnData object\n",
    "        publicness_file_path: String path to the _ppub_counts.csv file\n",
    "        cell_type_col: Name of the column in adata.obs with cell types\n",
    "        \n",
    "    Returns:\n",
    "        A new DataFrame 'classifier_table'\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Building Classifier Table ---\")\n",
    "\n",
    "    # --- Read and join publicness ---\n",
    "    print(\"Read publicness data\")\n",
    "    publicness_df = pd.read_csv(publicness_file_path)\n",
    "    print(publicness_df.head())\n",
    "\n",
    "    publicness_subset = (\n",
    "        publicness_df[['CDR3_match', 'publicness_score']]\n",
    "        .drop_duplicates(subset=['CDR3_match'], keep='first')\n",
    "    )\n",
    "    \n",
    "    classifier_table = pd.merge(\n",
    "        df_airrport_igblast_unified,\n",
    "        publicness_subset,\n",
    "        how='left',\n",
    "        left_on='CDR3_match',\n",
    "        right_on='CDR3_match'\n",
    "    )\n",
    "\n",
    "    print(\"Add gene expression matrix to classifier table\")\n",
    "    expr_df = adata.to_df()\n",
    "    expr_df = expr_df.reset_index().rename(columns={'index': 'CB'})\n",
    "    classifier_table = pd.merge(\n",
    "        classifier_table,\n",
    "        expr_df,\n",
    "        how='left',\n",
    "        on='CB'\n",
    "    )\n",
    "\n",
    "    #  Add Cell Type from annotation\n",
    "    print(\"Adding cell types\")\n",
    "    if cell_type_col in adata.obs.columns:\n",
    "        # Create a map with cleaned barcodes\n",
    "        cell_type_series = adata.obs[cell_type_col]\n",
    "        cell_type_map = cell_type_series.to_dict()\n",
    "        \n",
    "        # Map cell types using the cleaned CBs\n",
    "        classifier_table['cell_type'] = classifier_table['CB'].map(cell_type_map)\n",
    "    else:\n",
    "        print(f\"Warning: Cell type column '{cell_type_col}' not found in adata.obs. Skipping.\")\n",
    "        classifier_table['cell_type'] = None\n",
    "\n",
    "\n",
    "    print(\"Creating concatenated of CDR3_match and CB and then check if it exist in VDJ reference...\")\n",
    "    # concatenated of CDR3_match and CB in vdj reference\n",
    "    vdj_keys = set(\n",
    "        df_vdj['vdj_cdr3'].astype(str) + \"_\" + df_vdj['vdj_barcode'].astype(str)\n",
    "    )\n",
    "\n",
    "    # Create the new concatenated key column in classifier_table.\n",
    "    classifier_table['temp_key'] = (\n",
    "        classifier_table['CDR3_match'].astype(str) + \"_\" + classifier_table['CB'].astype(str)\n",
    "    )\n",
    "\n",
    "    # Use .isin() to check if each key is in the vdj_keys set.\n",
    "    classifier_table['is_in_vdj'] = classifier_table['temp_key'].isin(vdj_keys)\n",
    "\n",
    "    # Remove the temporary key column\n",
    "    classifier_table = classifier_table.drop(columns=['temp_key'])\n",
    "\n",
    "    print(\"\\nDone! Final table shape:\", classifier_table.shape)\n",
    "    print(\"Write classifier table head to csv file\")\n",
    "    classifier_table.head(5).to_csv(\"classifier_head.csv\", index=False)\n",
    "    \n",
    "    print(\"\\n--- Analysis of classifier_table ---\")\n",
    "\n",
    "    # 1. How many return true in 'in_vdj' column?\n",
    "    if 'is_in_vdj' in classifier_table.columns:\n",
    "        # .sum() treats True as 1 and False as 0\n",
    "        in_vdj_true_count = classifier_table['is_in_vdj'].sum()\n",
    "        print(f\"Total rows where 'in_vdj' is True: {in_vdj_true_count}\")\n",
    "    else:\n",
    "        print(\"Error: Column 'in_vdj' not found in classifier_table.\")\n",
    "\n",
    "    # 2. How many cdr3 there is for each cell type?\n",
    "    if 'cell_type' in classifier_table.columns and 'CDR3_match' in classifier_table.columns:\n",
    "        print(\"Unique CDR3 count per cell type:\")\n",
    "        \n",
    "        # Group by cell type, then count the unique CDR3s in each group\n",
    "        cdr3_per_cell_type = classifier_table.groupby('cell_type')['CDR3_match'].nunique()\n",
    "        \n",
    "        print(cdr3_per_cell_type)\n",
    "    else:\n",
    "        print(\"\\nError: Columns 'cell_type' or 'CDR3_match' not found, skipping CDR3 count per cell type.\")\n",
    "\n",
    "    print(\"--- Counting unique CDR3s for T cells in VDJ ---\")\n",
    " \n",
    "    # Check for necessary columns\n",
    "    required_cols = ['cell_type', 'is_in_vdj', 'CDR3_match']\n",
    "    if all(col in classifier_table.columns for col in required_cols):\n",
    "            \n",
    "        # 1. Filter for 'cell_type' containing \"T cells\"\n",
    "        #    Using na=False to safely handle potential NaN values\n",
    "        #    Using case=False to make the search case-insensitive\n",
    "        t_cell_filter = classifier_table['cell_type'].str.contains(\n",
    "            \"T cells\", \n",
    "            na=False, \n",
    "            case=False\n",
    "        )\n",
    "            \n",
    "        # 2. Filter for 'in_vdj' being True\n",
    "        in_vdj_filter = classifier_table['is_in_vdj'] == True\n",
    "            \n",
    "        # 3. Combine filters\n",
    "        combined_filter = t_cell_filter & in_vdj_filter\n",
    "\n",
    "        # 4. Apply filter and get the 'CDR3_match' column\n",
    "        filtered_cdr3s = classifier_table.loc[combined_filter, 'CDR3_match']\n",
    "\n",
    "        # 5. Count unique CDR3s\n",
    "        unique_cdr3_count = filtered_cdr3s.nunique()\n",
    "            \n",
    "        print(f\"Found {unique_cdr3_count} unique CDR3s that are 'in_vdj' and in cells containing 'T cells'.\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Error: 'classifier_table' is missing one or more required columns: {required_cols}\")\n",
    "\n",
    "\n",
    "    return classifier_table\n",
    "\n",
    "def build_model(classifier_table):\n",
    "    print(\"classifier_table:\", type(classifier_table))\n",
    "    # --- STEP 1: CREATE YOUR LABEL ---\n",
    "    print(\"Creating target label 'y'...\")\n",
    "    classifier_table['cell_type'] = classifier_table['cell_type'].fillna('')\n",
    "    is_t_cell = classifier_table['cell_type'].str.contains(\"T cells\", case=False)\n",
    "    is_in_vdj = classifier_table['is_in_vdj'] == True\n",
    "    classifier_table['label'] = is_t_cell & is_in_vdj\n",
    "\n",
    "    print(\"Label distribution:\")\n",
    "    label_counts = classifier_table['label'].value_counts()\n",
    "    print(label_counts)\n",
    "\n",
    "    # --- STEP 2: AUTOMATIC FEATURE SELECTION ---\n",
    "    print(\"Automatically selecting features 'X'...\")\n",
    "\n",
    "    # Define columns to EXCLUDE from features\n",
    "    exclude_cols = ['cdr3', 'CB', 'label', 'cell_type', 'is_in_vdj']\n",
    "    \n",
    "    # Select all columns that are numeric AND not in the exclude list\n",
    "    # This will automatically find all IgBlast scores, publicness, and gene columns\n",
    "    X = classifier_table.drop(columns=exclude_cols, errors='ignore').select_dtypes(include=np.number)\n",
    "    y = classifier_table['label']\n",
    "    \n",
    "    # Save column names for later\n",
    "    feature_names = X.columns.tolist()\n",
    "    print(f\"Found {len(feature_names)} numerical features (e.g., {feature_names[:3]}...)\")\n",
    "\n",
    "    # --- STEP 3: HANDLE MISSING DATA (Imputation) ---\n",
    "    # (Using median is robust to outliers, good for gene data)\n",
    "    print(\"Imputing missing data (NaNs)...\")\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "    # --- STEP 4: TRAIN-TEST SPLIT ---\n",
    "    print(\"Splitting data into train/test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_imputed, \n",
    "        y, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=y  # Ensures both train and test get a similar % of True/False\n",
    "    )\n",
    "\n",
    "    # --- STEP 5: SCALE FEATURES ---\n",
    "    print(\"Scaling features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # --- STEP 6: DEFINE & TRAIN MODELS (NEW) ---\n",
    "\n",
    "    # Calculate scale_pos_weight for XGBoost to handle imbalance\n",
    "    # (Count of negative class) / (Count of positive class)\n",
    "    scale_pos_weight = (y_train == False).sum() / (y_train == True).sum()\n",
    "    print(f\"Using scale_pos_weight for XGBoost: {scale_pos_weight:.2f}\")\n",
    "\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(\n",
    "            class_weight='balanced', # Handles imbalance for RF\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"XGBoost\": xgb.XGBClassifier(\n",
    "            scale_pos_weight=scale_pos_weight, # Handles imbalance for XGB\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # --- STEP 7: EVALUATE MODELS (NEW) ---\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        print(f\"--- Evaluating {name} ---\")\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        print(f\"\\nConfusion Matrix ({name}):\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "        print(f\"\\nClassification Report ({name}):\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['Not T-Cell', 'Is T-Cell']))\n",
    "\n",
    "        # Get Feature Importances\n",
    "        print(f\"\\nFeature Importances ({name}):\")\n",
    "        importances = model.feature_importances_\n",
    "        feature_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        })\n",
    "        print(feature_df.sort_values(by='importance', ascending=False).head(10))\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the entire pipeline.\n",
    "    \"\"\"\n",
    "    # --- Configuration ---\n",
    "    SC_DATA_DIR = '/dsi/efroni-lab/AIRRPORT/test_env/sc/GEX/runs/SRX10124718_gex/outs/filtered_feature_bc_matrix/'\n",
    "    AIRRPORT_PATH = '/home/ls/linoym/r_files/airrport/SRX10124718_sample/matched_SRX10124718_unaligned_reads_plusCBUB_trimmed_R2_collapsed.parquet'\n",
    "    VDJ_PATH = '/home/ls/linoym/r_files/airrport/SRX10124718_sample/P4_LNM_vdj_filtered_contig_annotations.csv'\n",
    "    IGBLAST_PATH = '/home/ls/linoym/r_files/airrport/SRX10124718_sample/igblast_SRX10124718.tsv'\n",
    "    LOG_FILE = \"scanpy_analysis.log\"\n",
    "    CELLTYPIST_MODEL = 'Immune_All_Low.pkl'\n",
    "    FINAL_ADATA_FILE = \"sc_rna_seq_processed.h5ad\"\n",
    "    PUBLICNESS_PATH = \"/home/ls/linoym/r_files/airrport/SRX10124718_ppub_counts.csv\"\n",
    "\n",
    "    log_handle = setup_logging(LOG_FILE)\n",
    "    \n",
    "    try:\n",
    "        # Run Pipeline\n",
    "        sc_rna_seq, df_airrport, df_vdj, df_airrport_igblast, cdr3_strict_conditions, df_airrport_igblast_unified = load_data(\n",
    "            SC_DATA_DIR,\n",
    "            AIRRPORT_PATH,\n",
    "            VDJ_PATH,\n",
    "            IGBLAST_PATH\n",
    "        )\n",
    "        # Preprocessing\n",
    "        sc_rna_seq = preprocess_sc_data(sc_rna_seq)\n",
    "        # Dimensionality Reduction & Clustering\n",
    "        sc_rna_seq = cluster_and_embed(sc_rna_seq)\n",
    "        # Save clustering plots\n",
    "        save_clustering_plots(sc_rna_seq)\n",
    "        # Annotate\n",
    "        sc_rna_seq = annotate_cell_types(sc_rna_seq, CELLTYPIST_MODEL)\n",
    "        save_annotation_results(sc_rna_seq)\n",
    "        # Final Save of the entire object\n",
    "        sc_rna_seq.write(FINAL_ADATA_FILE)\n",
    "        classifier_data = classifier_table(sc_rna_seq,df_airrport_igblast_unified,df_vdj ,PUBLICNESS_PATH,cell_type_col = 'predicted_cell_type')\n",
    "        analysis_logs(sc_rna_seq, df_airrport, df_vdj, df_airrport_igblast, cdr3_strict_conditions, classifier_data)\n",
    "        build_model(classifier_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"--- ERROR: Pipeline failed ---\", file=sys.stderr)\n",
    "        print(str(e), file=sys.stderr)\n",
    "        # Re-raise the exception after logging it\n",
    "        raise\n",
    "    finally:\n",
    "        # This block will run NO MATTER WHAT (success or error),\n",
    "        # ensuring your log file is always closed properly.\n",
    "        close_logging(log_handle)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airrport",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
